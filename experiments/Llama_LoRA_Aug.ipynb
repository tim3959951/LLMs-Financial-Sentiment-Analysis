{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import gradio as gr\n",
    "import torch\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as naw\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. First example:\n",
      "{'sentiment': 'neutral', 'text': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'}\n",
      "\n",
      "Dataset split:\n",
      "Train rows: 3876\n",
      "Validation rows: 485\n",
      "Test rows: 485\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning(Remove non-ASCII characters, keep x20~x7E)\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\x20-\\x7E]\", \"\", text)\n",
    "\n",
    "# Load CSV\n",
    "file_path = \"/Users/tim/Desktop/self-learning/LLM_tune/all-data.csv\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"full\": file_path},\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    column_names=[\"sentiment\", \"text\"]\n",
    ")[\"full\"]\n",
    "\n",
    "print(\"Dataset loaded. First example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Label mapping & cleaning\n",
    "def map_label_and_clean(example):\n",
    "    mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    example[\"text\"] = clean_text(example[\"text\"])\n",
    "    example[\"labels\"] = mapping[example[\"sentiment\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_label_and_clean)\n",
    "\n",
    "# Split data (80% train, 10% val, 10% test)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "temp_dataset = split_dataset[\"test\"]\n",
    "\n",
    "temp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "validation_dataset = temp_split[\"train\"]\n",
    "test_dataset = temp_split[\"test\"]\n",
    "\n",
    "print(\"\\nDataset split:\")\n",
    "print(\"Train rows:\", len(train_dataset))\n",
    "print(\"Validation rows:\", len(validation_dataset))\n",
    "print(\"Test rows:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/Users/tim/nltk_data\")\n",
    "\n",
    "# Synonym augmentation using WordNet\n",
    "syn_aug = naw.SynonymAug(\n",
    "    aug_src='wordnet',\n",
    "    aug_max=2,  # Max 2 word replacements per sentence\n",
    "    aug_p=0.3   # 30% probability per word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# English to Chinese translation\n",
    "en2zh_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "en2zh_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "\n",
    "# Chinese to English translation\n",
    "zh2en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "zh2en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "def back_translate_en_zh_en(text):\n",
    "    # English to Chinese\n",
    "    inputs = en2zh_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        zh_ids = en2zh_model.generate(**inputs)\n",
    "    zh_text = en2zh_tokenizer.decode(zh_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Chinese to English\n",
    "    inputs = zh2en_tokenizer(zh_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        en_ids = zh2en_model.generate(**inputs)\n",
    "    back_translated = zh2en_tokenizer.decode(en_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Add Random Swap Augmenter \n",
    "random_swap_aug = naw.RandomWordAug(\n",
    "    action=\"swap\",\n",
    "    aug_p=0.3,    # 30% probability of swapping words in a sentence\n",
    "    aug_min=1,    # Perform at least one swap\n",
    "    aug_max=2     # Perform at most two swaps\n",
    ")\n",
    "\n",
    "def random_swap_augmentation(text):\n",
    "    \"\"\"Perform word-level augmentation using nlpaug's RandomWordAug(action=\"swap\").\"\"\"\n",
    "    return random_swap_aug.augment(text)\n",
    "\n",
    "def news_headline_style(text):\n",
    "    \"\"\"Convert a sentence into 'news headline style' by removing stopwords.\"\"\"\n",
    "    words = text.split()\n",
    "    important_words = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(important_words)\n",
    "\n",
    "# Add a unify_text function to ensure augmented outputs are always strings\n",
    "def unify_text(aug_text):\n",
    "    \"\"\"\n",
    "    Convert augmented output (possibly a list) into a string.\n",
    "    If it is any other type (e.g., None), convert forcibly to string to avoid errors\n",
    "    when using Dataset.from_list().\n",
    "    \"\"\"\n",
    "    if isinstance(aug_text, list):\n",
    "        aug_text = \" \".join(aug_text)\n",
    "    elif not isinstance(aug_text, str):\n",
    "        aug_text = str(aug_text)\n",
    "    return aug_text\n",
    "\n",
    "# Perform multiple augmentations\n",
    "augmented_data = []\n",
    "for example in train_dataset:\n",
    "    text = example[\"text\"]\n",
    "    label = example[\"labels\"]\n",
    "\n",
    "    # Original text\n",
    "    augmented_data.append({\"text\": text, \"labels\": label})\n",
    "\n",
    "    # Synonym replacement (SynonymAug)\n",
    "    aug_text_syn = syn_aug.augment(text)\n",
    "    aug_text_syn = unify_text(aug_text_syn)\n",
    "    augmented_data.append({\"text\": aug_text_syn, \"labels\": label})\n",
    "\n",
    "    # Back translation (English → Chinese → English)\n",
    "    aug_text_bt = back_translate_en_zh_en(text)\n",
    "    aug_text_bt = unify_text(aug_text_bt)\n",
    "    augmented_data.append({\"text\": aug_text_bt, \"labels\": label})\n",
    "\n",
    "    # Random Swap Augmentation \n",
    "    aug_text_swap = random_swap_augmentation(text)\n",
    "    aug_text_swap = unify_text(aug_text_swap)\n",
    "    if aug_text_swap != text:\n",
    "        augmented_data.append({\"text\": aug_text_swap, \"labels\": label})\n",
    "\n",
    "    # News Headline Style\n",
    "    aug_text_news = news_headline_style(text)\n",
    "    aug_text_news = unify_text(aug_text_news)\n",
    "    if aug_text_news != text:\n",
    "        augmented_data.append({\"text\": aug_text_news, \"labels\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented train dataset size: 19363\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented data to dataset\n",
    "aug_train_dataset = Dataset.from_list(augmented_data)\n",
    "print(\"Augmented train dataset size:\", len(aug_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da6381c726a4098891c7055aa2d5acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized train first example:\n",
      "{'sentiment': 'neutral', 'text': \"Under the terms of the agreement , Bunge will acquire Raisio 's Keiju , Makuisa and Pyszny Duet brands and manufacturing plants in Finland and Poland .\", 'labels': 1, 'input_ids': [128000, 16648, 279, 3878, 315, 279, 9306, 1174, 426, 14208, 690, 21953, 432, 2852, 822, 364, 82, 6706, 64274, 1174, 40424, 9425, 64, 323, 393, 73445, 3919, 423, 14127, 16097, 323, 15266, 11012, 304, 37355, 323, 28702, 662, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Load model & tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Avoid padding error\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256  # Reduce memory usage\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_tokenized = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized train first example:\")\n",
    "print(train_tokenized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Llama 3.2-1B (No Fine-tuning) on Test Set ===\n",
      "{'eval_loss': 3.2506046295166016, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.1402061855670103, 'eval_f1': 0.07961807454339065, 'eval_runtime': 69.9626, 'eval_samples_per_second': 6.932, 'eval_steps_per_second': 6.932}\n",
      "\n",
      "Confusion Matrix (Baseline Llama):\n",
      "[[ 48   0  12]\n",
      " [240   0  42]\n",
      " [122   1  20]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baseline Llama evaluation\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics_baseline(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Load the original Llama model (without LoRA)\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Create inference-specific TrainingArguments\n",
    "baseline_inference_args = TrainingArguments(\n",
    "    output_dir=\"./llama_baseline_check\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Create Trainer (only for evaluation, not training)\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_inference_args,\n",
    "    compute_metrics=compute_metrics_baseline\n",
    ")\n",
    "\n",
    "# Evaluate the baseline model on the test set\n",
    "baseline_results = baseline_trainer.evaluate(eval_dataset=test_tokenized)\n",
    "print(\"\\n=== Baseline Llama 3.2-1B (No Fine-tuning) on Test Set ===\")\n",
    "print(baseline_results)\n",
    "\n",
    "# Confusion matrix\n",
    "pred_output = baseline_trainer.predict(test_tokenized)\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=-1)\n",
    "true_labels = pred_output.label_ids\n",
    "cm_base = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix (Baseline Llama):\")\n",
    "print(cm_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "LoRA model created for Llama 3.2-1B-Instruct.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 3.2-1B Instruct\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  # Sequence classification\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "print(\"LoRA model created for Llama 3.2-1B-Instruct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15504' max='38760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15504/38760 1:22:07 < 2:03:12, 3.15 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.911376</td>\n",
       "      <td>0.839175</td>\n",
       "      <td>0.835858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.710816</td>\n",
       "      <td>0.872165</td>\n",
       "      <td>0.872043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.863918</td>\n",
       "      <td>0.863710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>1.118943</td>\n",
       "      <td>0.861856</td>\n",
       "      <td>0.856818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json (Request ID: Root=1-67dc386a-1366cfc14fecb13f28611359;9f82760d-c6a2-4281-aff0-fcb1ac99351a)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json (Request ID: Root=1-67dc3d5c-1367d886487bd85c27d207fd;acf32025-d784-4343-85f6-2cc55ef02b17)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json (Request ID: Root=1-67dc4261-7a8ef688483394e655aa04cf;86664113-4b91-42c4-a3b4-f32fcc27e901)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json (Request ID: Root=1-67dc470b-753e281e42c2923d3a9a02a0;97004ad8-21f4-4a31-8417-ad6a6f7eff5c)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json (Request ID: Root=1-67dc470c-6d78c88e3821f02e221e8f74;98298438-5950-41af-9aad-65c403e4d7eb)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-lora-aug-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=1,  \n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,  # EarlyStoppingCallback will stop early if needed\n",
    "    learning_rate=2e-5,\n",
    "    # weight_decay=0.01,\n",
    "    fp16=False,  # False for Apple MPS\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Early Stopping (stops if no improvement for 2 evaluations)\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=validation_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(\"./llama3-lora-aug-finetuned\")\n",
    "print(\"Training complete. Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "{'eval_loss': 0.8149625062942505, 'eval_accuracy': 0.8494845360824742, 'eval_f1': 0.8503986345937338, 'eval_runtime': 67.7065, 'eval_samples_per_second': 7.163, 'eval_steps_per_second': 7.163, 'epoch': 4.0}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 48   9   3]\n",
      " [  5 243  34]\n",
      " [  2  20 121]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Confusion matrix\n",
    "predictions = trainer.predict(test_tokenized)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://99361ec9ae84643763.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://99361ec9ae84643763.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "\n",
    "device = torch.device(\"mps\")  \n",
    "model.to(device)\n",
    "\n",
    "# Labels & inference function\n",
    "label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_label = outputs.logits.argmax(dim=-1).item()\n",
    "    \n",
    "    return label_names[pred_label]\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Llama 3.2-1B-Instruct + LoRA + EarlyStopping\"\n",
    ")\n",
    "\n",
    "# Launch interface\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11228460,
     "sourceId": 89659,
     "sourceType": "competition"
    },
    {
     "datasetId": 6756286,
     "sourceId": 10874005,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 224423433,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
