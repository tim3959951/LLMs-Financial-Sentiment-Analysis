{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. First example:\n",
      "{'sentiment': 'neutral', 'text': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'}\n",
      "\n",
      "Dataset split:\n",
      "Train rows: 3876\n",
      "Validation rows: 485\n",
      "Test rows: 485\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning(Remove non-ASCII characters, keep x20~x7E)\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\x20-\\x7E]\", \"\", text)\n",
    "\n",
    "# Load CSV\n",
    "file_path = \"/Users/tim/Desktop/self-learning/LLM_tune/all-data.csv\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"full\": file_path},\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    column_names=[\"sentiment\", \"text\"]\n",
    ")[\"full\"]\n",
    "\n",
    "print(\"Dataset loaded. First example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Label mapping & cleaning\n",
    "def map_label_and_clean(example):\n",
    "    mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    example[\"text\"] = clean_text(example[\"text\"])\n",
    "    example[\"labels\"] = mapping[example[\"sentiment\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_label_and_clean)\n",
    "\n",
    "# Split data (80% train, 10% val, 10% test)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "temp_dataset = split_dataset[\"test\"]\n",
    "\n",
    "temp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "validation_dataset = temp_split[\"train\"]\n",
    "test_dataset = temp_split[\"test\"]\n",
    "\n",
    "print(\"\\nDataset split:\")\n",
    "print(\"Train rows:\", len(train_dataset))\n",
    "print(\"Validation rows:\", len(validation_dataset))\n",
    "print(\"Test rows:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0917b7a080b4f07b14d774f85f72842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized train first example:\n",
      "{'sentiment': 'neutral', 'text': \"Under the terms of the agreement , Bunge will acquire Raisio 's Keiju , Makuisa and Pyszny Duet brands and manufacturing plants in Finland and Poland .\", 'labels': 1, 'input_ids': [128000, 16648, 279, 3878, 315, 279, 9306, 1174, 426, 14208, 690, 21953, 432, 2852, 822, 364, 82, 6706, 64274, 1174, 40424, 9425, 64, 323, 393, 73445, 3919, 423, 14127, 16097, 323, 15266, 11012, 304, 37355, 323, 28702, 662, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Load model & tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Avoid padding error\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256  # Reduce memory usage\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_tokenized = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized train first example:\")\n",
    "print(train_tokenized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Llama 3.2-1B (No Fine-tuning) on Test Set ===\n",
      "{'eval_loss': 1.399227261543274, 'eval_model_preparation_time': 0.0011, 'eval_accuracy': 0.43711340206185567, 'eval_f1': 0.38738463044339044, 'eval_runtime': 85.6389, 'eval_samples_per_second': 5.663, 'eval_steps_per_second': 5.663}\n",
      "\n",
      "Confusion Matrix (Baseline Llama):\n",
      "[[ 12  47   1]\n",
      " [ 78 199   5]\n",
      " [ 41 101   1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baseline Llama evaluation\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics_baseline(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Load the original Llama model (without LoRA)\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Create inference-specific TrainingArguments\n",
    "baseline_inference_args = TrainingArguments(\n",
    "    output_dir=\"./llama_baseline_check\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Create Trainer (only for evaluation, not training)\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_inference_args,\n",
    "    compute_metrics=compute_metrics_baseline\n",
    ")\n",
    "\n",
    "# Evaluate the baseline model on the test set\n",
    "baseline_results = baseline_trainer.evaluate(eval_dataset=test_tokenized)\n",
    "print(\"\\n=== Baseline Llama 3.2-1B (No Fine-tuning) on Test Set ===\")\n",
    "print(baseline_results)\n",
    "\n",
    "# Confusion matrix\n",
    "pred_output = baseline_trainer.predict(test_tokenized)\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=-1)\n",
    "true_labels = pred_output.label_ids\n",
    "cm_base = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix (Baseline Llama):\")\n",
    "print(cm_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "LoRA model created for Llama 3.2-1B-Instruct.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 3.2-1B Instruct\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  # Sequence classification\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "print(\"LoRA model created for Llama 3.2-1B-Instruct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15504' max='38760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15504/38760 1:22:18 < 2:03:28, 3.14 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.911376</td>\n",
       "      <td>0.839175</td>\n",
       "      <td>0.835858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.710816</td>\n",
       "      <td>0.872165</td>\n",
       "      <td>0.872043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.863918</td>\n",
       "      <td>0.863710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>1.118943</td>\n",
       "      <td>0.861856</td>\n",
       "      <td>0.856818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Best model saved.\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-lora-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=1,  \n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,  # EarlyStoppingCallback will stop early if needed\n",
    "    learning_rate=2e-5,\n",
    "    # weight_decay=0.01,\n",
    "    fp16=False,  # False for Apple MPS\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Early Stopping (stops if no improvement for 2 evaluations)\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=validation_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(\"./llama3-lora-finetuned\")\n",
    "print(\"Training complete. Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "{'eval_loss': 0.8149625062942505, 'eval_accuracy': 0.8494845360824742, 'eval_f1': 0.8503986345937338, 'eval_runtime': 61.9439, 'eval_samples_per_second': 7.83, 'eval_steps_per_second': 7.83, 'epoch': 4.0}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 48   9   3]\n",
      " [  5 243  34]\n",
      " [  2  20 121]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Confusion matrix\n",
    "predictions = trainer.predict(test_tokenized)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://196f1cb18f414e5384.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://196f1cb18f414e5384.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "\n",
    "device = torch.device(\"mps\")  \n",
    "model.to(device)\n",
    "\n",
    "# Labels & inference function\n",
    "label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_label = outputs.logits.argmax(dim=-1).item()\n",
    "    \n",
    "    return label_names[pred_label]\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Llama 3.2-1B-Instruct + LoRA + EarlyStopping\"\n",
    ")\n",
    "\n",
    "# Launch interface\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11228460,
     "sourceId": 89659,
     "sourceType": "competition"
    },
    {
     "datasetId": 6756286,
     "sourceId": 10874005,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 224423433,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
